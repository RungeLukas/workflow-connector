# Signavio Workflow Accelerator Connector

Signavio Workflow Accelerator Connector is a RESTful web service which can be used to retrieve data from many external SQL Databases and forward this data to Signavio's Workflow Accelerator.

![Overview](docs/images/workflow-connector-overview.png?raw=true "Overview")

The connector is a simple executable that can be run on most servers In order to use the connector with Signavio's Workflow Accelerator, the connector must first be running on a server that is accessible to the public internet. After the connector is running and the database has been provisioned, a Workflow Accelerator administrator can [add](https://docs.signavio.com/userguide/workflow/en/integration/connectors.html#configuring-a-connector) the connector to a workspace under _Services & Connector_ menu entry. A process owner can then use the connector in a process to populate a drop down field dynamically using data from the database. More information about this can be found [here](https://docs.signavio.com/userguide/workflow/en/integration/connectors.html)

## Features

- Perform basic CRUD (create, read, update, delete) operations on database entries using a standard RESTful API
- Supports multiple SQL databases (MicrosoftSQL Server, MySQL, PostgresSQL)

## Deployment

The following examples will demonstrate how to deploy the workflow connector web service in the cloud, or on premise.

### On premise (bare metal) ###

Deploying the workflow connector on premise is as simple as copying the executable file, and related configuration files, to a directory on the server where it should be run. The following section demonstrate how this can be done in an ubuntu linux environment.

#### Installation ####

The connector can be downloaded from the [release page](https://github.com/signavio/workflow-connector/releases) for linux and windows platforms. Alternatively, the executable can be generated by compiling the source code as shown below.

##### Install from source #####

1. Download and install go from your distribution's package manager (for ubuntu `apt-get install go`) and make sure you are using version >= 1.11
2. Set the `$GOPATH` environment variable and add `$GOPATH/bin` to your `$PATH`. Here we assume that `$GOPATH` points to `~/go`. 

```sh
mkdir ~/go
echo "export GOPATH=$HOME/go" >> ~/.bashrc
echo "export PATH=\$PATH:\$GOPATH/bin" >> ~/.bashrc
source ~/.bashrc
```
3. Download and install the workflow-connector using the `go get` command on the command line.

```sh
go get -v github.com/signavio/workflow-connector
```
4. compile the source code into an architecture specific executable. Adjust your `$GOARCH` and `$GOOS` variables as needed.

```sh
# export the required environment variables
export CGO_ENABLED=1 GOARCH=amd64 GOOS=linux
# compile the code and generate the `workflow-connector` binary
go build -o workflow-connector cmd/wfadb/main.go
```

5. The executable is now located in `$GOPATH/github.com/signavio/workflow-connector/workflow-connector`. At this point you can copy the executable somewhere in your `$GOPATH`

```sh
# Here we assume ~/bin/ is in your $PATH
cp $GOPATH/github.com/signavio/workflow-connector/workflow-connector ~/bin/
```

6. If you have TLS enabled (highly recommended) and want to listen on port 443 without running the executable as root, you can set the proper permissions using the `setcap` command

```sh
setcap 'cap_net_bind_service=+ep' ~/bin/wfc
```
7. You can then run the executable with `systemd-run` as a non-root user and track the status of the workflow-connector process using the regular systemd tooling.

```sh
PORT=443 systemd-run --user ~/bin/wfc
```

#### Configuration ####

All program and environment specific configuration settings (like database connection information, username, password, etc.) should be stored in a directory named `config` which is located in the same directory where the `wfc` executable is running. This behaviour can be overriden by providing a `--config-dir` parameter to the `wfc` executable.

##### config.yaml #####

All configuration settings in `config.yaml` can also be specified as environment variables. For example, you can specify the database connection url by exporting the environment variable `DATABASE_URL=sqlserver://john:84mj29rSgHz@172.17.8.2?database=test`. This means that nested fields in the yaml file are delimited with a '_' (underscore) character when used in an environment variable. All configuration settings declared via environment variables will take precedence over the settings in your `config.yaml` file.

##### descriptor.json #####

The workflow connector also needs to know the schema of the data it will receive from the database. This is stored in the connector descriptor file `descriptor.json` and an example is provided in this repository. You can also refer to the [workflow documentation](https://docs.signavio.com/userguide/workflow/en/integration/connectors.html#connector-descriptor) for more information. 

##### HTTP basic auth #####

The webservice will only respond to clients using HTTP basic auth. This can be enabled by setting `tls.enabled = true` and providing valid TLS certificates in the `config.yaml` file. The username for HTTP basic auth is stored as plain text in `config.yaml` but the password is stored salted and hashed using [argon2](https://passlib.readthedocs.io/en/stable/lib/passlib.hash.argon2.html). You can use the following commands to generate a argon2 password hash using python.

1. Install passlib using python `pip`

```sh
pip install passlib
```

2. Use the python shell in the command line to generate an argon2 password hash with a digest size of 32 bytes

```python
from passlib.hash import argon2
argon2.using(digest_size=32).hash("password")
```

### In the cloud (using Heroku) ###

The following [screencast](https://drive.google.com/file/d/1V8Kizoka-5L-56SpqTRxshCBBDyerB7v/view?usp=sharing) will show how to use heroku to install, configure and deploy the workflow connector.

## Testing ##

You can test the deployment with a local sqlite database to make sure that the REST API is behaving properly. The following sections demonstrate how this can be done.

### Since everyone loves coffee ###

Let's assume we want to create a workflow that can instruct an intern on how to make coffee for the rest of our team. 
On a high level the intern would need to know what style of coffee we want, what the necessary ingredients are and how to properly use the machines to make the best batch of coffee possible. If we model these requirements in a database we could end up with a similar result as to whats depicted in the following diagramm.
 
![TODO](erm.png)

Translating this diagramm to plain english would result in the following:

- A recipe contains instructions for the intern to follow. Making a recipe requires *only one* piece of equipment. A recipe of course contains *many* ingredients.
- A piece of equipment can be used in *many* different recipes.
- The database keeps track of the ingredients in stock in the `inventory` table. When the intern is shown the ingredients necessary for a recipe he or she will also be shown if there is enough of these ingredients in stock.

This database model has been translated one to one in the `config/descriptor.json` file.

#### Okay now on to the prerequisites ####

1. Download and install sqlite

```sh
apt-get install sqlite
```

#### Populate the database ####

For testing purposes, we can execute the sqlite migration script provided in the `build/migrate-to-sqlite.sh` to create our schema and populate it with some example data. The equipment table should end up looking like this: 

**Equipment**

| id | name                        | acquisition_cost | purchase_date       |
|----|-----------------------------|------------------|---------------------|
|  1 | Bialetti Moka Express 6 cup |         25.95000 | 2017-12-12 12:00:00 |
|  2 | Sanremo CafÃ© Racer          |       8477.85000 | 2017-12-12 12:00:00 |
|  3 | Buntfink SteelKettle        |         39.95000 | 2017-12-12 12:00:00 |
|  4 | Copper Coffee Pot Cezve     |         49.95000 | 2017-12-12 12:00:00 |

#### Run the workflow connector ####

Before running the `workflow-connector` command, either edit the `config.yaml` file to include the database connection parameters and other settings, or export these settings as environment variables.

```sh
# Export environment variables
#
export PORT=:8080 DATABASE_URL=test.db DATABASE_DRIVER=sqlite3
#
# Run the connector
~/bin/workflow-connector
Listening on :8080

```

#### Exercise the REST API ####

Now we can test the functionality of the connector's REST API either in a new terminal, or using the following the [postman collection](TODO). All HTTP requests are sent using HTTP basic auth with the default username (`wfauser`) password (`Foobar`) combination here.

Go ahead and fetch the equipment with id 1 by sending a `HTTP GET` request to the connector using the `curl` command (you can `apt-get install curl` if `curl` is not yet installed):

```sh
curl --verbose --header "Authorization: Basic $(echo -n "wfauser:Foobar" | base64)" --request GET http://localhost:8080/equipment/1
# Response:
## Headers
> GET /equipment/1 HTTP/1.1
> Host: localhost:8080
> User-Agent: curl/7.55.1
> Accept: */*
> Authorization: Basic d2ZhdXNlcjpGb29iYXI=
>
< HTTP/1.1 200 OK
< Content-Type: application/json
< Date: Fri, 23 Mar 2018 21:33:47 GMT
< Content-Length: 595
<
## Data
{
  "cost": {
    "amount": 119,
    "currency": "EUR"
  },
  "equipmentMaintenance": [],
  "equipmentWarranty": [],
  "id": "1",
  "name": "Stainless Steel Cooling Spiral",
  "purchaseDate": "2017-09-07T12:00:00Z"
}
```

#### Insert a new equipment in the database ####

You can create a new product by sending a `HTTP POST` to the appropriate route (here `/equipment`)

```sh
curl --verbose --header "Authorization: Basic $(echo -n "wfauser:Foobar" | base64)" --request POST --data 'name=Malt+mill+550&acquisitionCost=1270&purchaseDate=2016-09-04+11:00:00' http://localhost:8080/equipment

# Response:
## Headers
> POST /equipment HTTP/1.1
> Host: localhost:8080
> User-Agent: curl/7.56.1
> Accept: */*
> Authorization: Basic d2ZhdXNlcjpGb29iYXI=
> Content-Type: application/x-www-form-urlencoded
> Content-Length: 45
>
* upload completely sent off: 45 out of 45 bytes
< HTTP/1.1 200 OK
< Content-Type: application/json
< Date: Fri, 23 Mar 2018 21:33:47 GMT
< Content-Length: 2
<
## Data
{
  "cost": {
    "amount": 1270,
    "currency": "EUR"
  },
  "equipmentMaintenance": [],
  "equipmentWarranty": [],
  "id": "5",
  "name": "Malt mill 550",
  "purchaseDate": "2017-09-04T11:00:00Z"
}
```

#### Updating an existing product ####

By sending a `HTTP PUT` to `/equipment` you can change existing entries. Let's go ahead an adjust the name of the malt mill we just added recently.


```sh
curl --verbose --header "Authorization: Basic $(echo -n "wfauser:Foobar" | base64)" --request PUT --data 'name=Malt+mill+400' http://localhost:8080/equipment/5

# Response:
## Headers
> PUT /equipment HTTP/1.1
> Host: localhost:8080
> User-Agent: curl/7.56.1
> Accept: */*
> Authorization: Basic d2ZhdXNlcjpGb29iYXI=
> Content-Type: application/x-www-form-urlencoded
> Content-Length: 45
>
* upload completely sent off: 45 out of 45 bytes
< HTTP/1.1 200 OK
< Content-Type: application/json
< Date: Fri, 23 Mar 2018 21:33:47 GMT
< Content-Length: 2
<
## Data
{
  "cost": {
    "amount": 1270,
    "currency": "EUR"
  },
  "equipmentMaintenance": [],
  "equipmentWarranty": [],
  "id": "5",
  "name": "Malt mill 400",
  "purchaseDate": "2017-09-04T11:00:00Z"
}
```

#### Deleting an existing product ####

TODO: deletion is not supported at the moment.

## Support

Any inquiries for support can be sent to [support](mailto:support@signavio.com). 

## Authors

The development team at Signavio with input from Stefano Da Ros and Peter Hilton 

## Licence

GNU General Public License Version 3
